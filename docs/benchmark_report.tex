\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{siunitx}
\usepackage{hyperref}
\usepackage{xurl}
\usepackage{longtable}
\sisetup{table-number-alignment=center,round-mode=places,round-precision=4}

\title{Noise-Aware Training for Analog-Inspired Output-Layer Encodings Under Inference Noise}
\author{Analog NN Benchmarking}
\date{}

\begin{document}
\maketitle

\begin{abstract}
We benchmark five MLP classifiers that differ only in the \textbf{output-layer} weight parameterization and noise injection: digital (baseline), multiplicative weight-noise (``amplitude''), multiplicative noise-aware training, cosine phase weights with phase noise, and phase noise-aware training. Across five datasets (MNIST, KMNIST, EMNIST Letters, CIFAR-10 flattened, Fashion-MNIST), we evaluate accuracy over fixed inference-noise grids and summarize per-dataset means and coarse robustness diagnostics (max--min over the grid). Under this setup (single training seed; single noise draw per \(\sigma\)), multiplicative noise-aware training nearly matches the digital baseline in mean accuracy (\num{0.8329} vs \num{0.8359}) while exhibiting reduced variation across the tested grids. Noise-aware training substantially improves the phase parameterization (mean \num{0.6156} \(\rightarrow\) \num{0.8002}), though phase remains weaker on CIFAR-10(flat) under the bounded \(\cos(\theta)\) model. We provide configuration details and reproducibility commands, and outline controls and multi-draw/multi-seed extensions for stronger robustness claims.
\end{abstract}

\section{Introduction}
Analog neural accelerators promise energy and latency benefits but face device noise and variability. This work isolates the \textbf{output layer only} and compares analog-inspired weight encodings (amplitude, phase) against a digital MLP baseline across multiple datasets. We ask: (1) How close can these output-layer encodings get to digital accuracy? (2) How much does noise-aware training help? (3) How do results vary with dataset difficulty?

\section{Models and Noise Modes}
Scope clarification: only the output layer is ``analog-modeled'' (amplitude or phase perturbations); hidden layers remain digital. Non-idealities are injected per-weight, i.i.d., at inference (and optionally during training). Claims are limited to these output encodings, not full-stack analog accelerators (no ADC/DAC, drift, saturation, or multi-layer analog noise accumulation).

For input \(x \in \mathbb{R}^d\) and class count \(C\):
\begin{itemize}
  \item \textbf{Digital}: standard MLP with hidden layers defined per dataset; logits are linear outputs over \(C\) classes.
  \item \textbf{Amplitude (multiplicative gain noise on signed weights)}: output weights \(W\) perturbed multiplicatively at inference:
  \[
    W_{\text{noisy}} = W \odot (1 + \epsilon), \quad \epsilon \sim \mathcal{N}(0, \sigma_{\text{amp}}^2).
  \]
  \item \textbf{Amplitude\_noiseaware}: same, but during training each forward pass samples \(\sigma_{\text{amp}}\) from a list (config \texttt{train\_noise\_list}).
  \item \textbf{Phase}: output weights encoded as \(\cos(\Theta)\) with additive phase noise at inference:
  \[
  \begin{aligned}
    &\Theta \in \mathbb{R}^{d \times C}, \qquad \epsilon \sim \mathcal{N}(0, \sigma_{\text{phase}}^2)\ \text{(elementwise)},\\
    &W = \cos(\Theta), \qquad W_{\text{noisy}} = \cos(\Theta + \epsilon), \qquad \text{logits} = x^\top W_{\text{noisy}} + b.
  \end{aligned}
  \]
  \item \textbf{Phase\_noiseaware}: same, but training samples \(\sigma_{\text{phase}}\) per forward pass from \texttt{train\_noise\_list}.
\end{itemize}
Loss is cross-entropy:
\[
  L = -\frac{1}{N} \sum_{i=1}^N \log \left(\text{softmax}(\text{logits}_i)_{y_i}\right).
\]
Optimization uses Adam with dataset-specific learning rates and epochs from \texttt{config.yml}. Batch size is 128. Seeds: \texttt{numpy}/\texttt{torch} set to 42. Device selection prefers Metal (mps) \(\rightarrow\) CUDA \(\rightarrow\) CPU.

\section{Datasets}
\begin{itemize}
  \item MNIST (10 classes, \(28 \times 28 \rightarrow 784\)).
  \item KMNIST (10 classes, \(28 \times 28 \rightarrow 784\)).
  \item EMNIST Letters (26 classes, \(28 \times 28 \rightarrow 784\)).
  \item CIFAR-10 (flattened, 10 classes, \(32 \times 32 \times 3 \rightarrow 3072\)).
  \item Fashion-MNIST (10 classes, \(28 \times 28 \rightarrow 784\)).
\end{itemize}
Architectures (\texttt{hidden\_dims}) and training hyperparameters are defined per dataset in \texttt{config.yml}.

\subsection{Key hyperparameters (from \texttt{config.yml})}
\begin{center}
\begin{tabular}{lcccccc}
\toprule
Dataset block & hidden\_dims & epochs & lr & batch & input\_dim & classes \\
\midrule
digits\_demo (MNIST) & [256, 128] & 40 & 0.005 & 128 & 784 & 10 \\
fashion\_complex (FMNIST) & [512, 256, 128] & 50 & 0.003 & 128 & 784 & 10 \\
kmnist\_benchmark & [512, 256] & 45 & 0.0035 & 128 & 784 & 10 \\
emnist\_letters\_benchmark & [512, 256, 128] & 55 & 0.003 & 128 & 784 & 26 \\
cifar10\_flat\_benchmark & [1024, 512, 256] & 65 & 0.0025 & 128 & 3072 & 10 \\
\bottomrule
\end{tabular}
\end{center}

\subsection{Noise grids (inference) and training noise lists}
Noise is injected per-weight, i.i.d. at inference. For noise-aware variants, each forward pass samples \(\sigma\) uniformly from the training list.
\begin{center}
\begin{tabular}{lp{4.5cm}p{4.5cm}}
\toprule
Dataset block & noise\_std (inference) & train\_noise\_list (phase/amplitude) \\
\midrule
digits\_demo & {[}0, 0.01, 0.02, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.4{]} & {[}0.01, 0.02, 0.05, 0.08, 0.1, 0.15, 0.2{]} \\
fashion\_complex & same as digits\_demo & same as digits\_demo \\
kmnist\_benchmark & {[}0, 0.01, 0.02, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.4{]} & {[}0.01, 0.02, 0.05, 0.08, 0.1, 0.15, 0.2{]} \\
emnist\_letters\_benchmark & {[}0, 0.01, 0.02, 0.05, 0.08, 0.12, 0.16, 0.2, 0.25, 0.3{]} & {[}0.01, 0.02, 0.05, 0.08, 0.12, 0.16, 0.2, 0.25{]} \\
cifar10\_flat\_benchmark & {[}0, 0.01, 0.02, 0.05, 0.08, 0.12, 0.16, 0.2, 0.25, 0.3{]} & {[}0.01, 0.02, 0.05, 0.08, 0.12, 0.16, 0.2, 0.25{]} \\
\bottomrule
\end{tabular}
\end{center}

\section{Experimental Setup}
\begin{itemize}
  \item \textbf{Training}: Adam; dataset-specific epochs, learning rates, hidden dimensions, class counts, batch size 128.
  \item \textbf{Noise sweeps}: inference noise standard deviations from each dataset's \texttt{noise\_std} list; each \(\sigma\) applied to the corresponding mode (phase or amplitude) and accuracy recorded.
  \item \textbf{Noise-aware training}: if \texttt{train\_noise\_list} is provided, each forward pass samples \(\sigma\) uniformly and injects noise as above.
  \item \textbf{Single draw per \(\sigma\)}: for each \(\sigma\), we draw one weight-perturbation tensor \(\epsilon\) and evaluate accuracy over the full test set with that fixed perturbation.
  \item \textbf{Units}: for phase models, \(\Theta\) and \(\sigma_{\text{phase}}\) are in radians; amplitude \(\sigma_{\text{amp}}\) is a dimensionless gain std.
  \item \textbf{Data splits}: training subset is \(80\%\) of the provided train set (chosen for runtime); evaluation on the official test set.
  \item \textbf{Transforms}: \texttt{ToTensor} + dataset-specific normalization + flatten.
  \item \textbf{Hardware}: auto-selected device (mps \(\rightarrow\) cuda \(\rightarrow\) cpu).
  \item \textbf{Determinism caveat}: single seed; MPS/CUDA may be non-deterministic; CPU can be used for stricter reproducibility (slower).
\end{itemize}

\subsection{Reproduction Steps}
\begin{enumerate}
  \item MNIST: \texttt{python src/run\_benchmark.py --config config.yml --csv results/mnist.csv --json results/mnist.json}
  \item KMNIST: \texttt{python src/run\_benchmark\_fashion.py --config config.yml --config-key kmnist\_benchmark --csv results/kmnist.csv --json results/kmnist.json}
  \item EMNIST Letters: \texttt{python src/run\_benchmark\_fashion.py --config config.yml --config-key emnist\_letters\_benchmark --csv results/emnist.csv --json results/emnist.json}
  \item CIFAR-10 (flat): \texttt{python src/run\_benchmark\_fashion.py --config config.yml --config-key cifar10\_flat\_benchmark --csv results/cifar10.csv --json results/cifar10.json}
  \item Fashion-MNIST: \texttt{python src/run\_benchmark\_fashion.py --config config.yml --config-key fashion\_complex --csv results/fmnist.csv --json results/fmnist.json}
  \item Analysis \& plots: \texttt{python scripts/analyze\_benchmark.py}
\end{enumerate}

\section{Results}

\subsection{Aggregate (mean over five datasets; per-dataset means are over the evaluation noise grid; spread = max--min on single-draw curves)}
Per-dataset mean accuracy is
\[
  a_k = \frac{1}{|\Sigma_k|} \sum_{\sigma \in \Sigma_k} \text{Acc}(k,\sigma),
\]
and the aggregate acc\_mean averages \(a_k\) over the five datasets. For the digital baseline, we report standard test accuracy (equivalently \(\Sigma_k = \{0\}\)) and do not inject inference noise.
\begin{table}[!htbp]
\centering
\begin{tabular}{l
S[table-format=1.4]
S[table-format=1.4]
S[table-format=1.4]
l}
\toprule
Model & \multicolumn{1}{c}{acc\_mean} & \multicolumn{1}{c}{acc\_min} & \multicolumn{1}{c}{acc\_max} & diff (spread) \\
\midrule
digital              & 0.8359 & 0.8359 & 0.8359 & N/A (no noise) \\
amplitude            & 0.8302 & 0.8158 & 0.8343 & 0.0185 \\
amplitude\_noiseaware & 0.8329 & 0.8310 & 0.8341 & 0.0031 \\
phase                & 0.6156 & 0.2125 & 0.8406 & 0.6281 \\
phase\_noiseaware     & 0.8002 & 0.6964 & 0.8207 & 0.1243 \\
\bottomrule
\end{tabular}
\caption{Aggregate accuracy and spread across five datasets. Per-dataset accuracies are averaged over the evaluation noise grid; spread is max--min on single-draw curves. Digital spread is undefined because no inference noise is applied. Amplitude denotes multiplicative gain noise on signed weights.}
\label{tab:aggregate}
\end{table}

\subsection{Per-dataset summaries (means, spreads; single seed, single noise draw per \(\sigma\))}
\begin{table}[!htbp]
\centering
\begin{tabular}{l
S[table-format=1.5]
S[table-format=1.5]
S[table-format=1.5]
S[table-format=1.5]
S[table-format=1.5]}
\toprule
Dataset & \multicolumn{1}{c}{digital} & \multicolumn{1}{c}{amplitude} & \multicolumn{1}{c}{amp\_noiseaware} & \multicolumn{1}{c}{phase} & \multicolumn{1}{c}{phase\_noiseaware} \\
\midrule
MNIST   & 0.9760 & 0.97297 & 0.97214 & 0.78415 & 0.94876 \\
KMNIST  & 0.8913 & 0.87966 & 0.88777 & 0.74540 & 0.85296 \\
EMNIST  & 0.9033 & 0.90091 & 0.89853 & 0.56895 & 0.89232 \\
CIFAR10 & 0.5189 & 0.51489 & 0.52177 & 0.31185 & 0.45151 \\
FMNIST  & 0.8899 & 0.88249 & 0.88441 & 0.66751 & 0.85530 \\
\bottomrule
\end{tabular}
\caption{Per-dataset mean accuracies (averaged over the evaluation noise grid).}
\label{tab:perdataset}
\end{table}

\begin{table}[!htbp]
\centering
\begin{tabular}{l
S[table-format=1.5]
S[table-format=1.5]
S[table-format=1.5]
S[table-format=1.5]
S[table-format=1.5]}
\toprule
Dataset & \multicolumn{1}{c}{digital} & \multicolumn{1}{c}{amplitude} & \multicolumn{1}{c}{amp\_noiseaware} & \multicolumn{1}{c}{phase} & \multicolumn{1}{c}{phase\_noiseaware} \\
\midrule
MNIST   & 0.0000 & 0.0081  & 0.0016 & 0.7033 & 0.1717 \\
KMNIST  & 0.0000 & 0.0087  & 0.0055 & 0.5855 & 0.1781 \\
EMNIST  & 0.0000 & 0.03385 & 0.00428 & 0.78808 & 0.0475 \\
CIFAR10 & 0.0000 & 0.0250  & 0.0018 & 0.4011 & 0.0590 \\
FMNIST  & 0.0000 & 0.0168  & 0.0024 & 0.6624 & 0.1652 \\
\bottomrule
\end{tabular}
\caption{Per-dataset spreads (max--min over the evaluation noise grid; single draw per \(\sigma\)).}
\label{tab:spreads}
\end{table}

\subsection{Figures}
\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{../figures/benchmark_mean_accuracy.png}
  \caption{Mean accuracy with min/max error bars across datasets.}
  \label{fig:meanacc}
\end{figure}
\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{../figures/benchmark_diff_range.png}
  \caption{Accuracy spread (max--min) per model.}
  \label{fig:spread}
\end{figure}
\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{../figures/benchmark_per_dataset.png}
  \caption{Per-dataset mean accuracy by model.}
  \label{fig:perdataset}
\end{figure}
\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{../figures/benchmark_confidence_intervals.png}
  \caption{Cross-dataset variability interval (t-based, descriptive) for per-dataset mean accuracy.}
  \label{fig:varinterval}
\end{figure}

\section{Statistical Perspective}
Let \(n=5\) datasets. We compute a t-based cross-dataset variability interval:
\[
  \bar{a} = \frac{1}{n} \sum_{k=1}^n a_k,\quad
  s = \sqrt{\frac{1}{n-1} \sum_{k=1}^n (a_k - \bar{a})^2},\quad
  \text{VarInterval} = \bar{a} \pm t_{0.975, n-1} \frac{s}{\sqrt{n}}.
\]
This reflects task difficulty variation, not a sampling CI. Because accuracies are bounded in \([0,1]\), these intervals are descriptive dispersion summaries only. Summary (from \texttt{results/benchmark\_stats\_summary.csv}):
\begin{itemize}
  \item digital: \(0.8359 \pm 0.2244\)
  \item amplitude: \(0.8302 \pm 0.2238\)
  \item amplitude\_noiseaware: \(0.8329 \pm 0.2205\)
  \item phase: \(0.6156 \pm 0.2343\)
  \item phase\_noiseaware: \(0.8002 \pm 0.2467\)
\end{itemize}
Accuracy deltas vs digital (means): amplitude \(-0.0057\); amplitude\_noiseaware \(-0.0030\); phase \(-0.2203\); phase\_noiseaware \(-0.0357\). Robustness: amplitude\_noiseaware spread is \(\sim 6\times\) smaller than amplitude; phase\_noiseaware spread is \(\sim 5\times\) smaller than phase. Digital spread is undefined because no noise was injected into the baseline; a digital+noise control would allow direct robustness comparison.

\section{Discussion}
Noise-aware training consistently tightens spreads and boosts phase performance. The mean gap between amplitude\_noiseaware and digital (\(\approx 0.003\)) is negligible relative to observed variation, making amplitude\_noiseaware the \emph{best-performing encoding under the tested assumptions}. Spread (max--min) is a coarse, outlier-sensitive diagnostic that depends on the chosen noise grid; we include it for quick comparison on a fixed grid rather than as a definitive robustness metric. Phase remains risky in this bounded \(\cos(\theta)\) parameterization with unwrapped Gaussian noise; conclusions about phase are conditional on this capacity and noise model (unwrapped Gaussian used here for simplicity in the small-\(\sigma\) regime; wrapped models like von Mises are more appropriate for larger phase noise). Dataset difficulty matters: small grayscale digits are forgiving; CIFAR-10(flat) primarily tests representation capacity of flattened MLPs.

\section{Implications for Output-Layer Encoding Choices}
\begin{itemize}
  \item Default to \textbf{amplitude\_noiseaware} within this output-layer perturbation model for robustness with minimal accuracy loss.
  \item If using phase, always train noise-aware; plain phase is too unstable in this bounded \(\cos(\theta)\) parameterization.
  \item For harder vision tasks, pair analog heads with stronger feature extractors (e.g., conv nets) before evaluating robustness.
\end{itemize}

\section{Threats to Validity}
Flattened CIFAR-10 should be read as a capacity/representation stress test for MLP heads rather than a canonical vision benchmark; convolutional features could alter rankings. Single-architecture MLP; deeper/wider networks may shift results. Formal significance testing across multiple seeds is not reported here; interpretations rely on effect sizes and spreads. Noise model may not capture all hardware non-idealities.

\section{Controls and Future Improvements}
\begin{itemize}
  \item Add \textbf{digital + noise} controls (inference and noise-aware training) to separate encoding effects from regularization by noise.
  \item Increase robustness rigor: multiple seeds; multiple noise draws per \(\sigma\); report worst-case accuracy and AUC over justified \(\sigma\) ranges.
  \item Phase capacity/noise model: consider \(w = \alpha \cos(\theta)\) or I/Q (\(a\cos\theta + b\sin\theta\)); consider wrapped noise for larger \(\sigma\).
  \item Use full train splits (or justify subsampling) and explore conv features for CIFAR-10.
\end{itemize}

\section{Related Work (brief)}
Phase/amplitude encodings in photonic and RF accelerators are sensitive to phase noise and amplitude drift; noise-aware or stochastic training has been proposed to improve robustness. Our findings align: injecting noise during training stabilizes inference under perturbations.

\section{Conclusion}
Under the tested assumptions (single seed; single noise draw per \(\sigma\); fixed noise grids), amplitude\_noiseaware offers the best observed balance of accuracy and robustness diagnostics across these datasets, approaching digital performance with minimal variation on the evaluated grids. Phase models require noise-aware training to be viable and still lag on challenging data in this bounded \(\cos(\theta)\) parameterization. Future work should incorporate convolutional feature extractors, per-dataset noise curricula, formal significance tests versus digital baselines, and digital+noise controls.

\section*{Acknowledgments}
We thank prior work on analog-inspired training and robustness; references omitted in this draft.

\section*{References (draft list)}
\small
\setlength{\LTpre}{0pt}
\setlength{\LTpost}{0pt}
\begin{longtable}{p{0.52\textwidth}p{0.12\textwidth}p{0.08\textwidth}p{0.22\textwidth}}
\toprule
Title & Year & Src. & Note \\
\midrule
\endfirsthead
\toprule
Title & {Year} & {Src.} & Note \\
\midrule
\endhead
\bottomrule
\endfoot
A Tutorial about Random Neural Networks in Supervised Learning (\url{http://arxiv.org/abs/1609.04846v1}) & 2016 & arXiv & Random neural networks / queueing view. \\
Predicting concentration levels of air pollutants by transfer learning and recurrent neural network (\url{http://arxiv.org/abs/2502.01654v1}) & 2025 & arXiv & Air pollution prediction. \\
Analog Alchemy: Neural Computation with In-Memory Inference, Learning and Routing (\url{http://arxiv.org/abs/2412.20848v1}) & 2024 & arXiv & In-memory analog neural computation and routing. \\
Masked Conditional Neural Networks for Audio Classification (\url{http://arxiv.org/abs/1803.02421v2}) & 2018 & arXiv & Conditional/masked neural networks for temporal signals. \\
The Deep Arbitrary Polynomial Chaos Neural Network (\url{http://arxiv.org/abs/2306.14753v1}) & 2023 & arXiv & Homogeneous chaos theory applied to deep networks. \\
A Neural Network-Evolutionary Computational Framework for RUL Estimation (\url{http://arxiv.org/abs/1905.05918v1}) & 2019 & arXiv & Remaining useful life estimation. \\
Memristors -- from In-memory computing to Neuromorphic Computing (\url{http://arxiv.org/abs/2004.14942v1}) & 2020 & arXiv & Survey of memristor-based computing. \\
Reservoir Memory Machines as Neural Computers (\url{http://arxiv.org/abs/2009.06342v2}) & 2020 & arXiv & Differentiable neural computers with explicit memory. \\
Adversarial Frontier Stitching for Remote Neural Network Watermarking (\url{http://arxiv.org/abs/1711.01894v2}) & 2017 & arXiv & Watermarking neural networks via adversarial stitching. \\
A Review on Neural Network Models of Schizophrenia and ASD (\url{http://arxiv.org/abs/1906.10015v2}) & 2019 & arXiv & Survey of NN models of ASD and schizophrenia. \\
Structure Is Not Enough: Leveraging Behavior for Neural Network Weight Reconstruction (\url{http://arxiv.org/abs/2503.17138v1}) & 2025 & arXiv & Weight reconstruction using behavioral cues. \\
Adiabatic Fine-Tuning of Neural Quantum States (\url{http://arxiv.org/abs/2503.17140v2}) & 2025 & arXiv & Neural quantum states and phase transitions in weight space. \\
Encoding binary neural codes in networks of threshold-linear neurons (\url{http://arxiv.org/abs/1212.0031v3}) & 2012 & arXiv & Encoding patterns via synaptic connections. \\
Recursive Self-Similarity in Deep Weight Spaces of Neural Architectures (\url{http://arxiv.org/abs/2503.14298v1}) & 2025 & arXiv & Fractal/coarse geometry perspective on deep weight spaces. \\
Development of a sensory-neural network for medical diagnosing (\url{http://arxiv.org/abs/1807.02477v1}) & 2018 & arXiv & Sensory-neural network for diagnostics. \\
Normalisation of Weights and Firing Rates in Spiking Neural Networks with STDP (\url{http://arxiv.org/abs/1910.00122v1}) & 2019 & arXiv & Spiking homeostasis and normalization. \\
Implementing a Bayes Filter in a Neural Circuit (\url{http://arxiv.org/abs/1512.07839v4}) & 2015 & arXiv & Bayesian filtering with neural circuits. \\
On functions computed on trees (\url{http://arxiv.org/abs/1904.02309v4}) & 2019 & arXiv & Hierarchical function compositions on trees. \\
Coherent states for compact Lie groups and their large-N limits (\url{http://arxiv.org/abs/1707.02355v1}) & 2017 & arXiv & Survey of heat-kernel coherent states. \\
Cognitive computation with autonomously active neural networks (\url{http://arxiv.org/abs/0901.3028v1}) & 2009 & arXiv & Self-sustained neural activity and cognition. \\
Coherent states in fermionic Fock-Krein spaces and their amplitudes (\url{http://arxiv.org/abs/1708.03047v2}) & 2017 & arXiv & Fermionic coherent states with indefinite inner products. \\
Review of Entangled Coherent States (\url{http://arxiv.org/abs/1112.1778v1}) & 2011 & arXiv & Survey of entangled coherent states. \\
Accumulate: An identity-based blockchain protocol (\url{http://arxiv.org/abs/2204.06878v2}) & 2022 & arXiv & DPoS blockchain with identity and cross-chain support. \\
Linear Delay-cell Design for Low-energy Delay Multiplication and Accumulation (\url{http://arxiv.org/abs/2007.13895v3}) & 2020 & arXiv & Low-energy MAC design. \\
MultiPLY: A Multisensory Object-Centric Embodied LLM in 3D World (\url{http://arxiv.org/abs/2401.08577v1}) & 2024 & arXiv & Multisensory object-centric embodied model. \\
AutoLungDx: A Hybrid Deep Learning Approach for Early Lung Cancer Diagnosis (\url{http://arxiv.org/abs/2305.00046v4}) & 2023 & arXiv & Hybrid 3D Res-U-Net/YOLOv5/ViT for lung cancer. \\
On the Capacity Region of the Two-User Interference Channel (\url{http://arxiv.org/abs/1302.1837v1}) & 2013 & arXiv & Interference channel capacity region. \\
A Multi-Stage Hybrid CNN-Transformer Network for Pediatric Lung Sound Classification (\url{http://arxiv.org/abs/2507.20408v2}) & 2025 & arXiv & Hybrid CNN-Transformer for lung sounds. \\
Interference Mitigation through Limited Transmitter Cooperation (\url{http://arxiv.org/abs/1004.5421v1}) & 2010 & arXiv & Cooperation strategies for interference mitigation. \\
A Self-Attention-Driven Deep Denoiser Model for Real Time Lung Sound Denoising (\url{http://arxiv.org/abs/2404.04365v3}) & 2024 & arXiv & Self-attention denoiser for lung sounds. \\
Call to Protect the Dark and Quiet Sky from Satellite Constellations (\url{http://arxiv.org/abs/2412.08244v2}) & 2024 & arXiv & Impact of satellite constellations on sky observations. \\
ResCap-DBP: Lightweight Residual-Capsule Network for DNA-Binding Protein Prediction (\url{http://arxiv.org/abs/2507.20426v1}) & 2025 & arXiv & Residual-capsule network for DBP prediction. \\
Piecewise Semi-Analytical Formulation for Coupled-Oscillator Systems (\url{http://arxiv.org/abs/2404.12780v1}) & 2024 & arXiv & Semi-analytical solutions for coupled oscillators. \\
How transferable are features in deep neural networks? (\url{http://arxiv.org/abs/1411.1792v1}) & 2014 & arXiv & Feature transferability in deep nets. \\
Parallel Neural Networks in Golang (\url{http://arxiv.org/abs/2304.09590v1}) & 2023 & arXiv & PNNs implemented in Go. \\
Dual Accuracy-Quality-Driven Neural Network for Prediction Interval Generation (\url{http://arxiv.org/abs/2212.06370v4}) & 2022 & arXiv & Uncertainty quantification with dual objectives. \\
Synchronization conditions in the Kuramoto model (\url{http://arxiv.org/abs/2007.04343v2}) & 2020 & arXiv & Synchronization conditions and seminorms. \\
Conformal Group Actions on Generalized Kuramoto Oscillators (\url{http://arxiv.org/abs/1812.06539v3}) & 2018 & arXiv & Group actions on generalized Kuramoto models. \\
Synchronization of Kuramoto Oscillators on Knots (\url{http://arxiv.org/abs/1104.3493v2}) & 2011 & arXiv & Knot-based oscillator synchronization. \\
Compute and Energy Consumption Trends in Deep Learning Inference (\url{http://arxiv.org/abs/2109.05472v2}) & 2021 & arXiv & Trends in compute and energy for DL inference. \\
Cold Start Latency in Serverless Computing (\url{http://arxiv.org/abs/2310.08437v2}) & 2023 & arXiv & Review of cold start latency in serverless. \\
Solving the Hamiltonian path problem with a light-based computer (\url{http://arxiv.org/abs/0708.1512v1}) & 2007 & arXiv & Optical approach to Hamiltonian path. \\
Quantum Computing: Vision and Challenges (\url{http://arxiv.org/abs/2403.02240v5}) & 2024 & arXiv & Vision and challenges in quantum computing. \\
Tierkreis: A Dataflow Framework for Hybrid Quantum-Classical Computing (\url{http://arxiv.org/abs/2211.02350v1}) & 2022 & arXiv & Dataflow framework for hybrid quantum-classical. \\
Synthetic Biology meets Neuromorphic Computing (\url{http://arxiv.org/abs/2504.10053v2}) & 2025 & arXiv & Bio-inspired olfactory perception system. \\
Double Robust Semi-Supervised Inference for the Mean (\url{http://arxiv.org/abs/2104.06667v2}) & 2021 & arXiv & Semi-supervised inference under MAR labeling. \\
A Comparative Study of Load Balancing Algorithms in Cloud Computing Environment (\url{http://arxiv.org/abs/1403.6918v1}) & 2014 & arXiv & Load balancing in cloud environments. \\
Universal Workers: A Vision for Eliminating Cold Starts in Serverless Computing (\url{http://arxiv.org/abs/2505.19880v2}) & 2025 & arXiv & Reducing cold starts in serverless computing. \\
Placement of Microservices-based IoT Applications in Fog Computing (\url{http://arxiv.org/abs/2207.05399v2}) & 2022 & arXiv & Taxonomy for fog computing placement. \\
Driven spin wave modes in XY ferromagnet (\url{http://arxiv.org/abs/1706.01619v6}) & 2017 & arXiv & Nonequilibrium phase transition in XY ferromagnets. \\
Room temperature reversible colossal volto-magnetic effect (\url{http://arxiv.org/abs/2308.04324v1}) & 2023 & arXiv & Volto-magnetic effect in oxide heterostructures. \\
Reversible Computing with Fast, Fully Static, Fully Adiabatic CMOS (\url{http://arxiv.org/abs/2009.00448v2}) & 2020 & arXiv & Energy-efficient reversible CMOS. \\
Monte Carlo study of the phase transitions in the classical XY ferromagnets (\url{http://arxiv.org/abs/2208.10109v8}) & 2022 & arXiv & Monte Carlo of anisotropic XY ferromagnets. \\
Supporting Multi-Cloud in Serverless Computing (\url{http://arxiv.org/abs/2209.09367v4}) & 2022 & arXiv & Multi-cloud strategies for serverless. \\
Bridging Phases at the Morphotropic Boundaries of Lead-Oxide Solid Solutions (\url{http://arxiv.org/abs/cond-mat/0511256v1}) & 2005 & arXiv & Piezoelectric solid solutions near morphotropic boundaries. \\
Graph Neural Networks Based Analog Circuit Link Prediction (\url{http://arxiv.org/abs/2504.10240v5}) & 2025 & arXiv & GNNs for analog circuit link prediction. \\
Partially Oblivious Neural Network Inference (\url{http://arxiv.org/abs/2210.15189v1}) & 2022 & arXiv & Oblivious inference for privacy. \\
A Metalearned Neural Circuit for Nonparametric Bayesian Inference (\url{http://arxiv.org/abs/2311.14601v1}) & 2023 & arXiv & Meta-learned neural circuit for Bayesian inference. \\
On the Accuracy of Analog Neural Network Inference Accelerators (\url{http://arxiv.org/abs/2109.01262v3}) & 2021 & arXiv & Accuracy analysis of analog NN accelerators. \\
DiffCkt: A Diffusion Model-Based Hybrid Neural Network Framework for Automatic Transistor-Level Generation of Analog Circuits (\url{http://arxiv.org/abs/2507.00444v2}) & 2025 & arXiv & Diffusion + hybrid NN for analog circuit generation. \\
The CEPC input for the European Strategy for Particle Physics - Accelerator (\url{http://arxiv.org/abs/1901.03169v1}) & 2019 & arXiv & CEPC accelerator design summary. \\
Applications of Particle Accelerators (\url{http://arxiv.org/abs/2407.10216v1}) & 2024 & arXiv & Overview of particle accelerator applications. \\
Accelerator design concept for future neutrino facilities (\url{http://arxiv.org/abs/0802.4023v2}) & 2008 & arXiv & Scoping study findings for future neutrino facilities. \\
Time-domain and Frequency-domain Signals and their Analysis (\url{http://arxiv.org/abs/2009.14544v2}) & 2020 & arXiv & Signals in time/frequency domains. \\
Fixed-Field Alternating-Gradient Accelerators (\url{http://arxiv.org/abs/1604.05221v1}) & 2016 & arXiv & Overview of FFAG accelerators for medical applications. \\
Training of mixed-signal optical convolutional neural network with reduced quantization level (\url{http://arxiv.org/abs/2008.09206v1}) & 2020 & arXiv & Mixed-signal optical CNN training. \\
Analog, In-memory Compute Architectures for Artificial Intelligence (\url{http://arxiv.org/abs/2302.06417v1}) & 2023 & arXiv & Energy-efficiency limits in analog in-memory computing. \\
HZO-based FerroNEMS MAC for In-Memory Computing (\url{http://arxiv.org/abs/2208.06499v1}) & 2022 & arXiv & Ferroelectric NEMS unimorph for low-energy MAC. \\
MRAM-based Analog Sigmoid Function for In-memory Computing (\url{http://arxiv.org/abs/2204.09918v1}) & 2022 & arXiv & Analog sigmoid using MRAM. \\
An Asynchronous Multi-Beam MAC Protocol for Multi-Hop Wireless Networks (\url{http://arxiv.org/abs/2111.10073v1}) & 2021 & arXiv & Multi-beam MAC for wireless networks. \\
Wireless sensors networks MAC protocols analysis (\url{http://arxiv.org/abs/1004.4600v1}) & 2010 & arXiv & MAC protocols for wireless sensor networks. \\
Energy Efficient Dual Designs of FeFET-Based Analog In-Memory Computing (\url{http://arxiv.org/abs/2410.19593v1}) & 2024 & arXiv & FeFET-based IMC with shift-add capability. \\
LionHeart: A Layer-based Mapping Framework for Heterogeneous Systems with Analog In-Memory Computing Tiles (\url{http://arxiv.org/abs/2401.09420v3}) & 2024 & arXiv & Mapping framework for analog IMC tiles. \\
Nonlinear Integrated Microwave Photonics (\url{http://arxiv.org/abs/1310.4897v1}) & 2013 & arXiv & Nonlinear optical effects on chip. \\
Crosstalk Reduction for Superconducting Microwave Resonator Arrays (\url{http://arxiv.org/abs/1206.5571v1}) & 2012 & arXiv & Crosstalk reduction in MKIDs. \\
Near-Field Microwave Microscopy of Materials Properties (\url{http://arxiv.org/abs/cond-mat/0001075v2}) & 2000 & arXiv & Near-field microwave microscopy. \\
Bell-state measurement and quantum teleportation using linear optics (\url{http://arxiv.org/abs/1304.1214v1}) & 2013 & arXiv & Bell-state measurement and teleportation schemes. \\
Enabling Scalable Photonic Tensor Cores with Polarization-Domain Photonic Computing (\url{http://arxiv.org/abs/2501.18886v1}) & 2025 & arXiv & Polarization-domain photonic tensor core. \\
Highly-coherent stimulated phonon oscillations in a multi-core optical fiber (\url{http://arxiv.org/abs/1811.06290v1}) & 2018 & arXiv & Coherent acoustic waves in multi-core fiber. \\
The COHERENT Experiment at the Spallation Neutron Source (\url{http://arxiv.org/abs/1509.08702v2}) & 2015 & arXiv & COHERENT CEvNS experiment overview. \\
CORE -- a COmpact detectoR for the EIC (\url{http://arxiv.org/abs/2209.00496v1}) & 2022 & arXiv & CORE detector proposal for EIC. \\
COHERENT Collaboration data release from the first detection of CEvNS on argon (\url{http://arxiv.org/abs/2006.12659v2}) & 2020 & arXiv & COHERENT argon CEvNS data release. \\
An optical fiber-based probe for photonic crystal microcavities (\url{http://arxiv.org/abs/physics/0406129v1}) & 2004 & arXiv & Fiber probe for photonic crystal cavities. \\
Photovoltaic-ferroelectric materials for the realization of all-optical devices (\url{http://arxiv.org/abs/2203.06515v1}) & 2022 & arXiv & Photovoltaic-ferroelectric materials for optical devices. \\
Frequency Ratio Measurements with 18-digit Accuracy Using a Network of Optical Clocks (\url{http://arxiv.org/abs/2005.14694v1}) & 2020 & arXiv & Optical clock frequency ratio measurements. \\
A Fast, robust algorithm for power line interference cancellation in neural recording (\url{http://arxiv.org/abs/1402.6862v2}) & 2014 & arXiv & Power line interference cancellation. \\
Understanding and mitigating noise in trained deep neural networks (\url{http://arxiv.org/abs/2103.07413v3}) & 2021 & arXiv & Noise in trained DNNs and mitigation. \\
Denoising Noisy Neural Networks: A Bayesian Approach with Compensation (\url{http://arxiv.org/abs/2105.10699v3}) & 2021 & arXiv & Bayesian denoising for noisy neural networks. \\
Noise and Bell's inequality (\url{http://arxiv.org/abs/1008.0667v2}) & 2010 & arXiv & Noise considerations in Bell tests. \\
Quantum and Classical Frontiers of Noise (\url{http://arxiv.org/abs/1612.03430v1}) & 2016 & arXiv & Survey of quantum/classical noise frontiers. \\
Noise based logic: why noise? (\url{http://arxiv.org/abs/1204.2545v4}) & 2012 & arXiv & Noise-based logic and randomness. \\
Decoherence and noise in open quantum system dynamics (\url{http://arxiv.org/abs/1605.07838v1}) & 2016 & arXiv & Decoherence and noise in open systems. \\
Instantaneous noise-based logic (\url{http://arxiv.org/abs/1004.2652v2}) & 2010 & arXiv & Deterministic logic with binary noise timefunctions. \\
Noise Dynamics in the Quantum Regime (\url{http://arxiv.org/abs/2311.17794v1}) & 2023 & arXiv & Time-dependent modulation of current fluctuations. \\
Simple Cracking of (Noise-Based) Dynamic Watermarking in Smart Grids (\url{http://arxiv.org/abs/2406.15494v3}) & 2024 & arXiv & Security analysis of noise-based watermarking. \\
Phase-Locked, Low-Noise, Frequency Agile Titanium: Sapphire Lasers (\url{http://arxiv.org/abs/physics/0507187v2}) & 2005 & arXiv & Phase-locked Ti:sapphire lasers with low noise. \\
Stokes' Drift and Hypersensitive Response with Dichotomous Markov Noise (\url{http://arxiv.org/abs/cond-mat/0501499v1}) & 2005 & arXiv & Stochastic Stokes' drift under dichotomous noise. \\
Shot noise for entangled and spin-polarized electrons (\url{http://arxiv.org/abs/cond-mat/0210498v1}) & 2002 & arXiv & Shot noise in entangled/spin-polarized transport. \\
The Data Conversion Bottleneck in Analog Computing Accelerators (\url{http://arxiv.org/abs/2308.01719v4}) & 2023 & arXiv & Data conversion limits in analog accelerators. \\
Analysis of Performance of Linear Analog Codes (\url{http://arxiv.org/abs/1511.05509v2}) & 2015 & arXiv & MSE performance bounds for linear analog codes. \\
Security of quantum key distribution with detection-efficiency mismatch (\url{http://arxiv.org/abs/1810.04663v3}) & 2018 & arXiv & Bounds for QKD with detector mismatch. \\
Performance Analysis of the Matrix Pair Beamformer with Matrix Mismatch (\url{http://arxiv.org/abs/1009.5979v4}) & 2010 & arXiv & Robustness of matrix pair beamformer. \\
The three and a half layers of dynamics : analog, digital, semi-digital, analog (\url{http://arxiv.org/abs/1106.0911v1}) & 2011 & arXiv & Perspective on analog/digital dynamics. \\
Are Bohmian trajectories real? (\url{http://arxiv.org/abs/quant-ph/0609172v2}) & 2006 & arXiv & Bohmian trajectories and classical mismatch. \\
Computation over Mismatched Channels (\url{http://arxiv.org/abs/1204.5059v2}) & 2012 & arXiv & Distributed computation over MAC with mismatch. \\
Superfluid Analog of the Davies-Unruh Effect (\url{http://arxiv.org/abs/gr-qc/0505005v1}) & 2005 & arXiv & Analog of Davies-Unruh in superfluid helium. \\
Semantic Communications with Discrete-time Analog Transmission: A PAPR Perspective (\url{http://arxiv.org/abs/2208.08342v3}) & 2022 & arXiv & Semantic communications with analog transmission. \\
Programmable photonic circuits (\url{https://doi.org/10.1038/s41566-020-0585-z}) & 2020 & bib & Overview of programmable photonic circuits. \\
Coupled oscillators for computing: A review and perspective (\url{https://doi.org/10.1063/1.5108897}) & 2020 & bib & Review of coupled oscillator computing. \\
Parallel convolutional processing using an integrated photonic tensor core (\url{https://doi.org/10.1038/s41586-020-03070-1}) & 2021 & bib & Photonic tensor core for convolutions. \\
Oscillatory neurocomputers with dynamic connectivity (\url{https://doi.org/10.1126/science.283.5408.1903}) & 1999 & bib & Oscillatory neurocomputer concept. \\
A 65nm 4.7TOPS/W 8bit CNN processor with mixed-signal computing (\url{https://doi.org/10.1109/ISSCC.2018.8310344}) & 2018 & bib & Mixed-signal CNN accelerator with calibration. \\
All-optical machine learning using diffractive deep neural networks (\url{https://doi.org/10.1126/science.aat8084}) & 2018 & bib & Diffractive optical layers performing inference. \\
Noise mitigation in analog in-memory computing for deep neural network accelerators (\url{https://doi.org/10.1109/JXCDC.2021.3090030}) & 2021 & bib & Noise mitigation for analog IMC accelerators. \\
Experimental demonstration of reservoir computing on a silicon photonics chip (\url{https://doi.org/10.1038/ncomms3541}) & 2014 & bib & Photonic reservoir computing demonstration. \\
Broadcast and weight: An integrated network for scalable photonic spike processing (\url{https://doi.org/10.1038/srep05522}) & 2014 & bib & Photonic weighting for neuromorphic spikes. \\
Optimal design for universal multiport interferometers (\url{https://doi.org/10.1364/OPTICA.3.001460}) & 2016 & bib & Mesh design for programmable interferometers. \\
Memory devices and applications for in-memory computing (\url{https://doi.org/10.1038/s41565-020-0655-z}) & 2020 & bib & Survey of memory devices for IMC. \\
Deep learning with coherent nanophotonic circuits (\url{https://doi.org/10.1038/nphoton.2017.93}) & 2017 & bib & Phase-programmable nanophotonic interferometer. \\
Neuromorphic photonic networks using silicon photonic weight banks (\url{https://doi.org/10.1038/s41598-017-06630-y}) & 2017 & bib & Photonic weight banks for coherent summation. \\
An oscillator-based Ising machine (\url{https://doi.org/10.1038/s41928-019-0300-0}) & 2019 & bib & Oscillator-based Ising machine. \\
Deep physical neural networks trained with backpropagation (\url{https://doi.org/10.1038/s41586-021-04223-6}) & 2022 & bib & Backpropagation through physical systems. \\
\end{longtable}
\normalsize

\appendix
\section{Full Tables}
\subsection{MNIST}
\begin{tabular}{lcccc}
\toprule
model & acc\_mean & acc\_min & acc\_max & diff \\
\midrule
digital & 0.976 & 0.976 & 0.976 & 0 \\
amplitude & 0.97297 & 0.9668 & 0.9749 & 0.0081 \\
amplitude\_noiseaware & 0.97214 & 0.971 & 0.9726 & 0.0016 \\
phase & 0.78415 & 0.2754 & 0.9787 & 0.7033 \\
phase\_noiseaware & 0.94876 & 0.802 & 0.9737 & 0.1717 \\
\bottomrule
\end{tabular}

\subsection{KMNIST}
\begin{tabular}{lcccc}
\toprule
model & acc\_mean & acc\_min & acc\_max & diff \\
\midrule
digital & 0.8913 & 0.8913 & 0.8913 & 0 \\
amplitude & 0.87966 & 0.8733 & 0.882 & 0.0087 \\
amplitude\_noiseaware & 0.88777 & 0.8846 & 0.8901 & 0.0055 \\
phase & 0.7454 & 0.3103 & 0.8958 & 0.5855 \\
phase\_noiseaware & 0.85296 & 0.7051 & 0.8832 & 0.1781 \\
\bottomrule
\end{tabular}

\subsection{EMNIST Letters}
\begin{tabular}{lcccc}
\toprule
model & acc\_mean & acc\_min & acc\_max & diff \\
\midrule
digital & 0.903317 & 0.903317 & 0.903317 & 0 \\
amplitude & 0.900913 & 0.873413 & 0.907260 & 0.033846 \\
amplitude\_noiseaware & 0.898534 & 0.895625 & 0.899904 & 0.004279 \\
phase & 0.568952 & 0.122837 & 0.910913 & 0.788077 \\
phase\_noiseaware & 0.892322 & 0.855 & 0.9025 & 0.0475 \\
\bottomrule
\end{tabular}

\subsection{CIFAR-10 (flattened)}
\begin{tabular}{lcccc}
\toprule
model & acc\_mean & acc\_min & acc\_max & diff \\
\midrule
digital & 0.5189 & 0.5189 & 0.5189 & 0 \\
amplitude & 0.51489 & 0.4967 & 0.5217 & 0.025 \\
amplitude\_noiseaware & 0.52177 & 0.5208 & 0.5226 & 0.0018 \\
phase & 0.31185 & 0.1246 & 0.5257 & 0.4011 \\
phase\_noiseaware & 0.45151 & 0.4057 & 0.4647 & 0.059 \\
\bottomrule
\end{tabular}

\subsection{Fashion-MNIST}
\begin{tabular}{lcccc}
\toprule
model & acc\_mean & acc\_min & acc\_max & diff \\
\midrule
digital & 0.8899 & 0.8899 & 0.8899 & 0 \\
amplitude & 0.88249 & 0.8687 & 0.8855 & 0.0168 \\
amplitude\_noiseaware & 0.88441 & 0.8829 & 0.8853 & 0.0024 \\
phase & 0.66751 & 0.2295 & 0.8919 & 0.6624 \\
phase\_noiseaware & 0.8553 & 0.714 & 0.8792 & 0.1652 \\
\bottomrule
\end{tabular}

\end{document}
